{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90debf4d-bcea-4601-9561-f01f1c1abb01",
   "metadata": {},
   "source": [
    "# GeoCLEWs\n",
    "\n",
    "**Revised code:** [Yalda Saedi](https://github.com/Ysaedi) <br />\n",
    "**Supervision:** [Taco Niet](https://github.com/tniet) <br />\n",
    "**Funding** [Mitacs](https://www.mitacs.ca/en) and [Catalyste+](https://www.catalysteplus.org/)\n",
    "\n",
    "Original code: [Alexandros Korkovelos](https://github.com/akorkovelos) <br />\n",
    "Supervision: [Abhishek Shivakumar](https://github.com/abhishek0208) & [Thomas Alfstad]() <br />\n",
    "Funding: UN EAPD/DESA <br />\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b284c8-8e21-4b92-a3c6-8cb6f46b09ac",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "GeoCLEWs is a versatile open-source script that offers a wide range of useful features for both developers and users. It is designed to efficiently collect, analyze, and process Global Agro-ecological Zones (GAEZ) data, including high-resolution land and water data, in an automated manner. The original code were rendered non-functional due to its incompatibility with the latest version of the GAEZ v4 dataset. The code has been subject to significant revisions and improvements aimed at increasing its efficiency and ease of use. Leveraging open-source tools and open datasets, the new script is designed to utilize GAEZ v4 datasets to process agro-climatic potential yield, crop water deficit, crop evapotranspiration, precipitation, and land cover.The script streamlines the detailed land and water processing steps required for Climate Land Energy Water systems (CLEWs) modelling.\n",
    "\n",
    "At the beginning of the script, users are required to provide inputs and customize the setup according to their specific project requirements. Once the necessary inputs are provided, the script will automatically execute the subsequent steps, including the collection of FAOSTAT (Food and Agriculture Organization of the United Nation) and GAEZ data based on the user-provided inputs. \n",
    "\n",
    "This notebook performs Five main analytical processes:\n",
    "\n",
    "- **Part 1**: Initialization and configuration.\n",
    "- **Part 2**: FAOSTAT and GAEZ data collection and preparation.\n",
    "- **Part 3**: Generating clusters.\n",
    "- **Part 4**: Geospatial attributes extraction to clusters.\n",
    "- **Part 5**: Calculating key summary statistics generate outputs for further use in CLEWs modelling.\n",
    "\n",
    "Each part below is accompanied by a more detailed explanation of the involved processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab101e01-c6bf-4b77-99e5-910e9ae18fa8",
   "metadata": {},
   "source": [
    "# Part 1 : Initialization and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01861b68-0cdf-43e9-9c0b-cecb40a9f10f",
   "metadata": {},
   "source": [
    "# 1.1. Importing Essential Modules \n",
    "\n",
    "To begin, it is necessary to import the required modules/libraries. For more information on the environment setup, please consult the README file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f824aa-63ba-4997-a0eb-907ec5ff1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary Python modules or libraries\n",
    "\n",
    "# Numerical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# Spatial\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterstats import zonal_stats\n",
    "from geojson import Feature, Point, FeatureCollection\n",
    "import json\n",
    "from shapely.geometry import Polygon, Point\n",
    "import gdal\n",
    "from pyproj import CRS\n",
    "from rasterio.mask import mask\n",
    "\n",
    "#Plotting\n",
    "import ipywidgets\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# System & Other\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab4915-29d8-4108-af4f-ac255d7e8ce2",
   "metadata": {},
   "source": [
    "# 1.2. User Configuration  \n",
    "This is the only part where the user needs to input values. The rest of the process will be automatically run based on the provided inputs. The code is designed with flexibility, allowing users to make changes and take advantage of customized settings at any time during the execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa940d56-693b-4975-b23a-16ecb9cd5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide specifications for the project \n",
    "\n",
    "Full_name = \"Kenya\"             # Specify the full name of the country you wish to analyze (e.g. Kenya).\n",
    "\n",
    "RCP = \"RCP4.5\"                  # Choose your preferred RCP (Representative Concentration Pathway) from the available options: RCP2.6, RCP4.5, RCP6.0, RCP8.5.\n",
    "\n",
    "\n",
    "admin_level = 1                 # 0 for national processing, admin level 0.\n",
    "                                # 1 for processing at admin level 1.\n",
    "\n",
    "#Aggregating regions    \n",
    "aggregate = False               # If you are processing at admin level 1 and there are more than 7 regions, it is advised to group regions to speed up computational processing.                            \n",
    "                                # \"False\" indicates no aggregation, \"True\" performs aggregation. \n",
    "                                # Set to True to aggregate into groups.\n",
    "\n",
    "region_per_group = 10           # Number of regions to be grouped into one group.\n",
    "Extract = False                 # In the case of aggregation, there is an option to extract a specific region and avoid aggregation. \n",
    "Ext_region = \"TAI\"              # It is required to input the first 3 letters of the specific region e.g. \"TAI\" refers to Taita Taveta county in Kenya.\n",
    "                                \n",
    "\n",
    "\n",
    "## Coordinate and projection systems\n",
    "crs_WGS84 = CRS(\"EPSG:4326\")    # For analysis, the original WGS84 coordinate system is used. \n",
    "crs_proj = CRS(\"EPSG:21037\")    # Please provide proper projection system based on the geographical location of the selected country. \n",
    "                                # In this example EPSG 21037 refers to a specific projection system called the \"Kenya Mapping Grid\".\n",
    "                                # More info: http://epsg.io/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000898b-3778-4094-865d-c7ceb61adda5",
   "metadata": {},
   "source": [
    "## 1.3. Directory Initialization and Structure\n",
    "\n",
    "For easier replication of the code you may used the following directory structure:\n",
    "\n",
    "* **~root/Data/input**    (a directory where your input data)\n",
    "* **~root/global_raster_input**   (directory for GAEZ raster layers with global coverage.  Precipitation and land cover rasters have already downloaded in the global_raster_input while agro-climatic potential yield, crop water deficit, and crop evapotranspiration with global coverage will be downloaded automatically in the following steps based on user input)\n",
    "* **~root/cropped_raster_input** (a directory for cropped GAEZ global raster data based on administraty boundry of selected country to reduce computational complexity)\n",
    "\n",
    "Results will be store in two automatically generated directories:\n",
    "* **~root/Data/output**   (directory for general output)\n",
    "* **~root/Data/output/summary_stats**   (a directory where the tabular outputs data and graphs are stored)\n",
    "\n",
    "**Note!** In case you decide to use a different structure please revise the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635483db-29f4-408a-89c2-f007b3896d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "ROOT_DIR = os.path.abspath(os.curdir)\n",
    "data_folder = \"Data\"\n",
    "in_path = os.path.join(ROOT_DIR, data_folder + \"\\\\\"+ 'input')\n",
    "in_path_raster = os.path.join(ROOT_DIR, 'global_raster_input')\n",
    "out_path_raster = os.path.join(ROOT_DIR, 'cropped_raster_input')\n",
    "if not os.path.exists(out_path_raster):\n",
    "    try:\n",
    "        os.makedirs(out_path_raster)\n",
    "    except OSError as exc: \n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "out_path = os.path.join(ROOT_DIR, data_folder + \"\\\\\"+ 'output')\n",
    "if not os.path.exists(out_path):\n",
    "    try:\n",
    "        os.makedirs(out_path)\n",
    "    except OSError as exc: \n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "    try:\n",
    "        os.makedirs(out_path)\n",
    "    except OSError as exc:                             \n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "summary_stats_path = out_path + \"\\\\\" + \"summary_stats\"\n",
    "if not os.path.exists(summary_stats_path):\n",
    "    try:\n",
    "        os.makedirs(summary_stats_path)\n",
    "    except OSError as exc:                                         # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "\n",
    "# 3 letter ISO code of the selected country\n",
    "code = pd.read_csv('Country_code.csv')                            # More info: https://www.nationsonline.org/oneworld/country_code_list.htm                                        \n",
    "code_name = code[code['Full_name']== Full_name]\n",
    "country_name = code_name.iloc[0]['country_code']            \n",
    "                        \n",
    "            \n",
    "# supporting vector point name\n",
    "shp_nm = \"{}_data.shp\".format(country_name)\n",
    "    \n",
    "#administrative boundary\n",
    "admin0_nm = '{}_adm0.shp'.format(country_name)                     # administrative boundaries - national analysis\n",
    "\n",
    "# Name of final result file\n",
    "output_nm = \"{}_vector_admin{}_clusters\".format(country_name, admin_level)\n",
    "result_nm = \"{}_vector_admin{}_clusters_with_attributes\".format(country_name, admin_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d6281-1212-4337-b3be-0e4ba3b68ddc",
   "metadata": {},
   "source": [
    "# Part 2 : FAOSTAT and GAEZ Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ce500-ccda-469d-a1b7-a6a8393388dd",
   "metadata": {},
   "source": [
    "## 2.1. FAOSTAT Collection and Preparation\n",
    "Finding top 10 crops in terms of harvested area from the latest statistical database provided by Food and Agriculture Organization of the United Nations (FAOSTAT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e109f-9191-46e1-9fd9-a982ee2abf7a",
   "metadata": {},
   "source": [
    "### 2.1.1. Retrieve Top 10 Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450780a9-1857-422e-8252-66d2ceb61481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the FAOSTAT file\n",
    "data = pd.read_csv('FAOSTAT_2020.csv')\n",
    "filtered_data = data[data['Area'] == Full_name]                \n",
    "\n",
    "# Sorting based on the harvested area in descending order and get top 10 rows\n",
    "# Retrieve data according to the user-defined country\n",
    "top_10_values = filtered_data.nlargest(10, 'Value')\n",
    "all_crops = top_10_values['Item'].tolist() \n",
    "\n",
    "main_crops= all_crops[:5]\n",
    "other_crops = all_crops[5:]\n",
    "\n",
    "display(Markdown(' **Top 5 crops considering harvested area are:** {}'.format(main_crops)))\n",
    "display(Markdown(' **Crops ranked from fsix to ten in the top 10 FAO dataset are:** {}'.format(other_crops)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8ad3c5-a0f7-4e57-8e40-528a2a8cb7db",
   "metadata": {},
   "source": [
    "### 2.1.2. FAOSTAT Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97ea11-ad00-4abc-bf52-0edc1e14d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAO correction: 3 letter naming convention per crop considering CLEWs naming format\n",
    "\n",
    "Crop_code = pd.read_csv('Crop_code.csv')\n",
    "crop_name = []  \n",
    "\n",
    "for item in main_crops:\n",
    "    matching_rows = Crop_code[Crop_code['Name']==item]\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        crop_name.extend(matching_rows['Code'].tolist())  \n",
    "\n",
    "other_crop_name = []  \n",
    "\n",
    "for item in other_crops:\n",
    "    matching_rows = Crop_code[Crop_code['Name']==item]\n",
    "    \n",
    "    if not matching_rows.empty:\n",
    "        other_crop_name.extend(matching_rows['Code'].tolist())  \n",
    "\n",
    "\n",
    "#Adding \"prc\" refering to annual precipitation\n",
    "crop_name = crop_name + [\"prc\"]\n",
    "\n",
    "display(Markdown(' **Based on 3-letter naming, the main crop list from the FAOSTAT is :** {}'.format(crop_name)))\n",
    "display(Markdown(' **Based on 3-letter naming, additional crop list from the FAOSTAT is :** {}'.format(other_crop_name)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13aae90-70c2-4696-a3aa-88c8b07cad75",
   "metadata": {},
   "source": [
    "## 2.2. GAEZ Data Collection and Preparation\n",
    "\n",
    "GeoCLEWs collects TIFF data from the Global Agro-Ecological Zones data portal for the following variables: agro-climatic potential yield, crop water deficit, and crop evapotranspiration. Precipitation, and land cover have already downloaded in the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7139d53-04eb-4d71-a919-1d5225d8440e",
   "metadata": {},
   "source": [
    "### 2.2.1. GAEZ Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953d7ac-a26f-4ab5-9202-0cfb30f3421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import agro-climatic potential yield \n",
    "yld_High_input = pd.read_csv('GAEZ_yld_High_Input.csv')\n",
    "yld_Low_input = pd.read_csv('GAEZ_yld_Low_Input.csv')\n",
    "\n",
    "#Import crop water deficit\n",
    "cwd_High_input = pd.read_csv('GAEZ_cwd_High_Input.csv')\n",
    "cwd_Low_input = pd.read_csv('GAEZ_cwd_Low_Input.csv')\n",
    "\n",
    "#Import crop evapotranspiration \n",
    "evt_High_input = pd.read_csv('GAEZ_evt_High_Input.csv')\n",
    "evt_Low_input = pd.read_csv('GAEZ_evt_Low_Input.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff35412-37d9-45c8-a028-f6bab88262b4",
   "metadata": {},
   "source": [
    "### 2.2.2. GAEZ Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7f4c4-72db-461e-a24b-af7f369d16e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a new column for water supply\n",
    "yld_High_input['New Water Supply'] = yld_High_input['Water Supply'].apply(lambda x: 'Irrigation' if 'irrigation' in x else 'Rain-fed')\n",
    "yld_Low_input['New Water Supply'] = yld_Low_input['Water Supply'].apply(lambda x: 'Irrigation' if 'irrigation' in x else 'Rain-fed')\n",
    "\n",
    "cwd_High_input['New Water Supply'] = cwd_High_input['Water Supply'].apply(lambda x: 'Irrigation' if 'irrigation' in x else 'Rain-fed')\n",
    "cwd_Low_input['New Water Supply'] = cwd_Low_input['Water Supply'].apply(lambda x: 'Irrigation' if 'irrigation' in x else 'Rain-fed')\n",
    "\n",
    "evt_High_input['New Water Supply'] = evt_High_input['Water Supply'].apply(lambda x: 'Irrigation' if 'irrigation' in x else 'Rain-fed')\n",
    "evt_Low_input['New Water Supply'] = evt_Low_input['Water Supply'].apply(lambda x: 'Irrigation' if 'irrigation' in x else 'Rain-fed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70387684-69ff-47ab-8859-b9e3a2778b90",
   "metadata": {},
   "source": [
    "### 2.2.3. GAEZ Data Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3530c2-2190-435a-9f11-f746ddaee1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def GAEZ_naming (dataset, filename):\n",
    "    \n",
    "    dataset['New Crop'] = dataset['Crop'].apply(lambda x: Crop_code.loc[Crop_code['GAEZ_name'] == x, 'Code'].values[0] if x in Crop_code['GAEZ_name'].values else 'Nan')\n",
    "    dataset.to_csv(filename, index=False)\n",
    "    \n",
    "GAEZ_naming (yld_High_input, 'New_yld_High_input.csv')    \n",
    "GAEZ_naming (yld_Low_input, 'New_yld_Low_input.csv') \n",
    "GAEZ_naming (cwd_High_input, 'New_cwd_High_input.csv') \n",
    "GAEZ_naming (cwd_Low_input, 'New_cwd_Low_input.csv') \n",
    "GAEZ_naming (evt_High_input, 'New_evt_High_input.csv') \n",
    "GAEZ_naming (evt_Low_input, 'New_evt_Low_input.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12da16-2d15-4526-914d-c5e0f8884064",
   "metadata": {},
   "source": [
    "### 2.2.4. GAEZ Data Filtering according to User Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e63d3c-1dff-4e2a-b423-1f56508cac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering in accordance with user-defined RCP \n",
    "Filtered_yld_High_input = yld_High_input[yld_High_input['RCP'] == RCP]\n",
    "Filtered_cwd_High_input = cwd_High_input[cwd_High_input['RCP'] == RCP]\n",
    "Filtered_evt_High_input = evt_High_input[evt_High_input['RCP'] == RCP]\n",
    "\n",
    "#filtering based on with user-defined crops\n",
    "def GAEZ_List(dataframe, crop_list, column):\n",
    "    List = pd.DataFrame()\n",
    "    for crop in crop_list:\n",
    "        if dataframe[column].str.contains(crop).any():\n",
    "            List = pd.concat([List, dataframe[dataframe[column].str.contains(crop)]])\n",
    "    return List\n",
    "\n",
    "\n",
    "\n",
    "Main_yld_High=GAEZ_List(Filtered_yld_High_input, crop_name, \"New Crop\")\n",
    "Other_yld_High=GAEZ_List(Filtered_yld_High_input, other_crop_name, \"New Crop\")\n",
    "Main_yld_Low=GAEZ_List(yld_Low_input, crop_name, \"New Crop\")\n",
    "Other_yld_Low=GAEZ_List(yld_Low_input, other_crop_name, \"New Crop\")\n",
    "\n",
    "Main_cwd_High=GAEZ_List(Filtered_cwd_High_input, crop_name, \"New Crop\")\n",
    "Other_cwd_High=GAEZ_List(Filtered_cwd_High_input, other_crop_name, \"New Crop\")\n",
    "Main_cwd_Low=GAEZ_List(cwd_Low_input, crop_name, \"New Crop\")\n",
    "Other_cwd_Low=GAEZ_List(cwd_Low_input, other_crop_name, \"New Crop\")\n",
    "\n",
    "Main_evt_High=GAEZ_List(Filtered_evt_High_input, crop_name, \"New Crop\")\n",
    "Other_evt_High=GAEZ_List(Filtered_evt_High_input, other_crop_name, \"New Crop\")\n",
    "Main_evt_Low=GAEZ_List(evt_Low_input, crop_name, \"New Crop\")\n",
    "Other_evt_Low=GAEZ_List(evt_Low_input, other_crop_name, \"New Crop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38234d27-4b43-4425-b579-8d4cc50fc44c",
   "metadata": {},
   "source": [
    "### 2.2.5. Downloading and Storing GAEZ Raster Files in a clewsy-Compatible Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c6e55-ee62-4797-9a42-e047154904b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_URL(dataframe, column, folder_name):\n",
    "    for index, row in dataframe.iterrows():\n",
    "        url = str(row[column])\n",
    "        filename = str(row['New Crop']) + ' ' + ('cwd' if str(row['Name'].split('_')[-1]) == \"wde\" else 'evt' if str(row['Name'].split('_')[-1]) == \"eta\" else str(row['Name'].split('_')[-1])) + ' ' + str(row['New Water Supply']) + ' ' + str(row['Input Level'])\n",
    "        file_path = os.path.join(folder_name, filename + '.tif')\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "\n",
    "\n",
    "download_URL(Main_yld_High, 'Download URL', in_path_raster)\n",
    "download_URL(Other_yld_High, 'Download URL', in_path_raster)\n",
    "download_URL(Main_yld_Low, 'Download URL', in_path_raster)\n",
    "download_URL(Other_yld_Low, 'Download URL', in_path_raster)\n",
    "\n",
    "download_URL(Main_cwd_High, 'Download URL', in_path_raster)\n",
    "download_URL(Other_cwd_High, 'Download URL', in_path_raster)\n",
    "download_URL(Main_cwd_Low, 'Download URL', in_path_raster)\n",
    "download_URL(Other_cwd_Low, 'Download URL', in_path_raster)\n",
    "\n",
    "download_URL(Main_evt_High, 'Download URL', in_path_raster)\n",
    "download_URL(Other_evt_High, 'Download URL', in_path_raster)\n",
    "download_URL(Main_evt_Low, 'Download URL', in_path_raster)\n",
    "download_URL(Other_evt_Low, 'Download URL', in_path_raster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b006f0-bac1-47f4-912f-da7555be3da8",
   "metadata": {},
   "source": [
    "# Part 3: Generating Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1bbef-8a99-4c7b-88c9-34f032db98d1",
   "metadata": {},
   "source": [
    "## 3.1. Generating Georeferenced Point Grid from Shapefile\n",
    "Considering the resolution of GAEZ raster files, it is recommended to set spacing to 0.09 decimal degrees resulting in a detailed land and water analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e56c728-e25d-4cb8-bc15-f81125ae2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a GeoDataFrame from the attributes and geometry of the shapefile\n",
    "shapefile = gpd.read_file(in_path + \"\\\\\" + shp_nm)\n",
    "shapefile = shapefile.to_crs(crs_WGS84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e788c4-8c45-4030-bcd0-8113ed132c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating point grid\n",
    "spacing = 0.09\n",
    "xmin, ymin, xmax, ymax = shapefile.total_bounds\n",
    "\n",
    "xcoords = [i for i in np.arange(xmin, xmax, spacing)]\n",
    "ycoords = [i for i in np.arange(ymin, ymax, spacing)]\n",
    "\n",
    "pointcoords = np.array(np.meshgrid(xcoords, ycoords)).T.reshape(-1, 2) \n",
    "points = gpd.points_from_xy(x=pointcoords[:,0], y=pointcoords[:,1])\n",
    "grid = gpd.GeoSeries(points, crs=shapefile.crs)\n",
    "grid.name = 'geometry'\n",
    "\n",
    "\n",
    "#only points inside administrative boundary:\n",
    "gridinside = gpd.sjoin(gpd.GeoDataFrame(grid), shapefile[['geometry']], how=\"inner\")\n",
    "\n",
    "#Plot georeferenced point grid\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "shapefile.plot(ax=ax, alpha=0.7, color=\"pink\", edgecolor='red', linewidth=3)\n",
    "grid.plot(ax=ax, markersize=30, color=\"blue\")\n",
    "gridinside.plot(ax=ax, markersize=15, color=\"yellow\")\n",
    "file_path = os.path.join(out_path, data_folder + \"_PointGrid.png\")\n",
    "plt.savefig(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac422e-63a6-4932-a061-2530de94d482",
   "metadata": {},
   "source": [
    "## 3.2. Converting Points to Polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4016d7-6d8f-4094-bca1-84e38ae7d661",
   "metadata": {},
   "source": [
    "A regular grid point is created across the entire area of interest in the previous step. Georeferenced points have unique latitude and longitude. In this step,  square buffer-based polygons are created around each point. This allows further flexibility in the extraction of raster values using stats. The buffered polygon shall split \"equally\" the area between neighbor points; therefore, the buffer used shall be the half of the distance between two neighbor points. This, in turn depends on the location of the AoI on earth and the projection system used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89231fdc-321c-403b-8966-609b0adc4f45",
   "metadata": {},
   "source": [
    "### 3.2.1. Spatial Join\n",
    "Assigning the same administrative region as defined in the GeoDataFrame to the 'cluster' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13dee75-a000-430b-b18b-66323856b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroids \n",
    "clustered_gdf = gridinside\n",
    "clustered_gdf = clustered_gdf.to_crs(crs_WGS84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f8c6d-280a-4180-83ba-9f30c6f77a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns to cluster\n",
    "clustered_gdf.rename(columns={'index_right': 'cluster'}, inplace=True)\n",
    "\n",
    "# Convert cluster column to string\n",
    "clustered_gdf.cluster = clustered_gdf.cluster.astype(str).replace('0', 'NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa19b18-49e0-4a38-b793-300be8fcc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the left dataframe\n",
    "clustered_gdf = clustered_gdf.reset_index(drop=True)\n",
    "\n",
    "if admin_level == 0:\n",
    "    # Perform the spatial join\n",
    "    clustered_gdf = gpd.sjoin(clustered_gdf, shapefile[[\"geometry\", \"GID_0\"]], op='within').drop(['cluster'], axis=1)\n",
    "    \n",
    "    # Rename the 'GID_0' column to 'cluster'\n",
    "    clustered_gdf.rename(columns={'GID_0': 'cluster'}, inplace=True)\n",
    "else:\n",
    "    # Perform the spatial join\n",
    "    clustered_gdf = gpd.sjoin(clustered_gdf, shapefile[[\"geometry\", \"NAME_1\"]], op='within').drop(['cluster'], axis=1)\n",
    "    \n",
    "    # Rename the 'NAME_1' column to 'cluster'\n",
    "    clustered_gdf.rename(columns={'NAME_1': 'cluster'}, inplace=True)\n",
    "\n",
    "# Print the first 5 rows of the joined GeoDataFrame\n",
    "clustered_gdf.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb292f-a77e-474f-8951-548e9aa5c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column based on first 3 letters of the 'cluster' column\n",
    "clustered_gdf['new_cluster'] = clustered_gdf['cluster'].apply(lambda x:  x[:3]).str.upper()\n",
    "clustered_gdf = clustered_gdf.rename(columns={'cluster': 'old_cluster'})\n",
    "clustered_gdf = clustered_gdf.rename(columns={'new_cluster': 'cluster'})\n",
    "clustered_gdf = clustered_gdf.drop(columns=['old_cluster'])\n",
    "clustered_gdf.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00142605-5c06-4029-8626-9fc0374eaa20",
   "metadata": {},
   "source": [
    "### 3.2.2. Generating Polygon\n",
    "Creating Polygons From Georeferenced Clustered Grid Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c6eba-44e2-4733-96dd-de242da21d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buffer value used should be half the distance between two adjacent points, which in turn is dependent on the location of the Area of Interest (AoI) on Earth and the projection system being used.\n",
    "buffer_value = 0.045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667cf6bb-8157-4aff-9f93-6291c754c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap_style refers to the type of geometry generated; 3=square (see shapely documectation for more info -- https://shapely.readthedocs.io/en/stable/manual.html)\n",
    "\n",
    "clustered_gdf['geometry'] = clustered_gdf.apply(lambda x:\n",
    "                                                          x.geometry.buffer(buffer_value, cap_style=3), axis=1)\n",
    "\n",
    "clustered_gdf.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f9ded7-11d1-44aa-9be6-8f3f1465d527",
   "metadata": {},
   "source": [
    "**Note!** Several features are not classified into a cluster. While points away of the administrative borders will be cut out of the analysis, some points right next to the outer administrative borders might create inconsistency when calculating area. In the following section we are dealing with this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc3ede-0e2d-460e-82bb-35fa64672ec2",
   "metadata": {},
   "source": [
    "## 3.3. Total Area Re-Estimation & Calibration\n",
    "\n",
    "This step estimates and calibrates the area (in square km) based on the area provided by the admin layer used in the analysis (e.g. clipping). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74627075-a14c-47e5-a153-96876e3cbc30",
   "metadata": {},
   "source": [
    "### 3.3.1. Area Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46aa733-74e3-40af-9f7e-58086eb1c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read admin layer as GeoDtaFrame\n",
    "admin = gpd.read_file(in_path + \"\\\\\" + admin0_nm)\n",
    "\n",
    "#Project to proper crs\n",
    "admin = admin.to_crs(crs_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cdcd60-2a7f-44f2-aa5a-8891bf2aac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clustered_GAEZ_gdf = clustered_gdf\n",
    "final_clustered_GAEZ_gdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e8323-6887-4ac3-bf20-624ad6fd9ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project datasets to proper crs\n",
    "final_clustered_GAEZ_gdf_prj = final_clustered_GAEZ_gdf.to_crs(crs_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87a9160-000c-4112-a883-d8b1f3924a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column for area calculation\n",
    "final_clustered_GAEZ_gdf_prj[\"sqkm\"] = final_clustered_GAEZ_gdf_prj['geometry'].area/10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2102b3c-e9cd-42ae-8d5f-488098739a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiplier(estimated, official):\n",
    "    if official == estimated:\n",
    "        return 1\n",
    "    try:\n",
    "        return  official / estimated\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cfb66e-66dd-4106-b456-b5df859577a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_area = final_clustered_GAEZ_gdf_prj.sqkm.sum()\n",
    "official_area = admin.geometry.area.sum()/10**6\n",
    "\n",
    "# Estimate column multipler\n",
    "multiplier = get_multiplier(estimated_area, official_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1044e-9404-4dce-8bca-4d158329e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clustered_GAEZ_gdf_prj.sqkm = final_clustered_GAEZ_gdf_prj.sqkm * multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92695874-a7aa-4167-b588-e895ae51ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Our modelling exercise yields a total area of {0:.1f} sqkm for the country\".format(estimated_area))\n",
    "print (\"The admin layer indicates {0:.1f} sqkm\".format(official_area))\n",
    "print (\"After calibration the total area is set at {0:.1f} sqkm\".format(final_clustered_GAEZ_gdf_prj.sqkm.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884b0e6d-9ece-492b-a88c-18832eced3f9",
   "metadata": {},
   "source": [
    "### 3.3.2. Final Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8522a2fd-638a-4d02-8b57-a7071a0684c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revert to original crs\n",
    "final_clustered_GAEZ_gdf = final_clustered_GAEZ_gdf_prj.to_crs(crs_WGS84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced60daa-4610-4442-8f8f-e8d8906ae722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final check\n",
    "final_clustered_GAEZ_gdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bbe9ca-554c-4f86-a140-f1adbb7fbda7",
   "metadata": {},
   "source": [
    "### 3.3.3. Export as GeoPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e76963-24c3-4ddc-bbf5-7a9ac42c4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clustered_GAEZ_gdf.to_file(os.path.join(out_path,\"{c}.gpkg\".format(c=output_nm)),driver=\"GPKG\")\n",
    "print (\"Part 3 complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d297dd-d80e-4e4a-b130-323858efc6fc",
   "metadata": {},
   "source": [
    "# Part 4: Geospatial Attributes Extraction to Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c7319-6fa2-4a4f-9e6d-88c68e79b611",
   "metadata": {},
   "source": [
    "The functions employed in the fourth part extract values from TIFF-formatted GAEZ raster files, and assign them as attributes to the clusters based on their spatial locations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71548c-4139-4769-9115-655d7ed27063",
   "metadata": {},
   "source": [
    "## 4.1. Clipping GAEZ Raster Files\n",
    "The administrative boundary of the study area is used to clip the GAEZ raster files with global coverage, which leads to a reduction in the computational processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5157d-2879-4f3e-8677-da9bb10b2c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "admin = admin.to_crs(crs_WGS84)\n",
    "for i in os.listdir(in_path_raster):\n",
    "    with rasterio.open(os.path.join(in_path_raster, i)) as src:\n",
    "        # Get the admin's CRS\n",
    "        admin_crs = admin.crs\n",
    "        \n",
    "        # Get the geometry of the admin\n",
    "        admin_geom = admin.geometry.values[0]\n",
    "        \n",
    "        # Crop the raster based on the admin's geometry\n",
    "        out_image, out_transform = mask(src, [admin_geom], crop=True)\n",
    "        \n",
    "        # Update the metadata of the cropped raster\n",
    "        out_meta = src.meta.copy()\n",
    "        out_meta.update({\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": out_image.shape[1],\n",
    "            \"width\": out_image.shape[2],\n",
    "            \"transform\": out_transform,\n",
    "            \"crs\": admin_crs\n",
    "        })\n",
    "        \n",
    "        # Write the cropped raster to the output directory\n",
    "        out_path_tif_crop = os.path.join(out_path_raster, i)\n",
    "        with rasterio.open(out_path_tif_crop, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473569d7-cfe6-4f77-956c-63d852b22d36",
   "metadata": {},
   "source": [
    "## 4.2. Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd20d1-f349-4265-a9b3-1e113471d90e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Processing Continuous/Numerical Rasters\n",
    "def processing_raster_con(path, raster, prefix, method, clusters):\n",
    "    \"\"\"\n",
    "    This function calculates stats for numerical rasters and attributes them to the given vector features. \n",
    "    \n",
    "    INPUT: \n",
    "    name: string used as prefix when assigning features to the vectors\n",
    "    method: statistical method to be used (check documentation)\n",
    "    clusters: the vector layer containing the clusters\n",
    "    \n",
    "    OUTPUT:\n",
    "    geojson file of the vector features including the new attributes\n",
    "    \"\"\"\n",
    "\n",
    "    raster=rasterio.open(path + '\\\\' + raster)\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        stats=[method],\n",
    "        prefix=prefix, geojson_out=True, all_touched=True)\n",
    "    \n",
    "    print(\"{} processing completed at\".format(prefix), datetime.datetime.now())\n",
    "    return clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504cd13-ab40-434d-8c1e-8a23944b6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Categorical/Discrete Rasters\n",
    "def processing_raster_cat(path, raster, prefix, clusters):\n",
    "    \"\"\"\n",
    "    This function calculates stats for categorical rasters and attributes them to the given vector features. \n",
    "    \n",
    "    INPUT: \n",
    "    path: the directory where the raster layer is stored \n",
    "    raster: the name and extention of the raster layer \n",
    "    prefix: string used as prefix when assigning features to the vectors\n",
    "    clusters: the vector layer containing the clusters\n",
    "    \n",
    "    OUTPUT:\n",
    "    geojson file of the vector features including the new attributes\n",
    "    \"\"\"    \n",
    "    raster=rasterio.open(path + '\\\\' + raster)\n",
    "    \n",
    "    clusters = zonal_stats(\n",
    "        clusters,\n",
    "        raster.name,\n",
    "        categorical=True,\n",
    "        prefix=prefix, geojson_out=True, all_touched=True)\n",
    "    \n",
    "    print(\"{} processing completed at\".format(prefix), datetime.datetime.now())\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955b5d0-0320-455f-beaa-8e142d84427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting geojson to GeoDataFrame\n",
    "def geojson_to_gdf(workspace, geojson_file):\n",
    "    \"\"\"\n",
    "    This function returns a GeoDataFrame for a given geojson file\n",
    "    \n",
    "    INPUT: \n",
    "    workplace: working directory\n",
    "    geojson_file: geojson layer to be convertes\n",
    "    crs: projection system in epsg format (e.g. 'EPSG:21037')\n",
    "    \n",
    "    OUTPUT:\n",
    "    GeoDataFrame\n",
    "    \"\"\"\n",
    "    output = workspace + r'\\placeholder.geojson'\n",
    "    with open(output, \"w\") as dst:\n",
    "        collection = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": list(geojson_file)}\n",
    "        dst.write(json.dumps(collection))\n",
    "  \n",
    "    clusters = gpd.read_file(output)\n",
    "    os.remove(output)\n",
    "    \n",
    "    print(\"cluster created a new at\", datetime.datetime.now())\n",
    "    return clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34230443-3b0c-4e28-941c-5e06a03df6b9",
   "metadata": {},
   "source": [
    "## 4.3. Collecting Raster Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df6feb-f68c-432d-9c29-f3e6b62650ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files with tif extension and assign their name into two list for discrete and continuous datasets\n",
    "raster_files_dis = []\n",
    "raster_files_con =[]\n",
    "\n",
    "for i in os.listdir(out_path_raster):\n",
    "    if (\"ncb\" in i) and i.endswith('.tif'):\n",
    "        with rasterio.open(out_path_raster + '\\\\' + i) as src:\n",
    "            raster_files_dis.append(i)            \n",
    "    else:\n",
    "        with rasterio.open(out_path_raster  + '\\\\' + i) as src:\n",
    "            data = src.read()\n",
    "            raster_files_con.append(i)\n",
    "\n",
    "                \n",
    "# keep only unique values -- Not needed but just in case there are dublicates\n",
    "raster_files_con = list(set(raster_files_con))\n",
    "raster_files_dis = list(set(raster_files_dis))\n",
    "                \n",
    "print (\"We have identified {} continuous raster(s):\".format(len(raster_files_con)),\"\\n\",)\n",
    "for raster in raster_files_con:\n",
    "    print ( \"*\", raster)\n",
    "    \n",
    "print (\"\\n\", \"We have identified {} discrete raster(s):\".format(len(raster_files_dis)),\"\\n\",)\n",
    "for raster in raster_files_dis:\n",
    "    print ( \"*\", raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e92083-1b69-437a-995a-7385a8a03ad0",
   "metadata": {},
   "source": [
    "## 4.4. Extracting Raster Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88612e-c6e2-435b-834e-8872079f39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = final_clustered_GAEZ_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243243ae-ff0a-43c6-a6d5-58f842efb9e6",
   "metadata": {},
   "source": [
    "### 4.4.1. Continuous Datasets (e.g. precipitation, crop evapotranspiration etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dccfeb5-cd4a-47e3-9ae7-40975565e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raster in raster_files_con:\n",
    "    prefix = raster.rstrip(\".tif\")\n",
    "    prefix = prefix + \"_\"\n",
    "    \n",
    "    # Calling the extraction function for continuous layers\n",
    "    clusters = processing_raster_con(out_path_raster, raster, prefix, \"mean\", clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5528f8ee-9bab-4740-88bb-9c2c79eccbdf",
   "metadata": {},
   "source": [
    "### 4.4.2. Categorical Datasets (e.g. Land cover type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919accef-f46c-44cc-b153-364e6093cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raster in raster_files_dis:\n",
    "    prefix = raster.rstrip(\".tif\")\n",
    "    prefix = prefix.rstrip('_ncb')\n",
    "    \n",
    "    # Calling the extraction function for discrete layers\n",
    "    clusters = processing_raster_cat(out_path_raster, raster, prefix, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8767be10-4341-4881-a67a-661626a65e58",
   "metadata": {},
   "source": [
    "### 4.4.3. Converting the GeoJSON File to GeoDataFrame\n",
    "\n",
    "**NOTE** In case you get an Driver Error for reading the geojson file into a GeoDataFrame, this might be cause due to attribution of \"inf\" or \"-inf\" value in one of the attributes. This is related to the way python handles json (see fix [here](https://stackoverflow.com/questions/17503981/is-there-a-way-to-override-pythons-json-handler)). An \"easy\" fix is that you import the geojson into Qgis and replace the erroneous value(s) manually. This is not ideal but it will do the job. In that case, save the updated geojson file and use the second (commented) line below to import into a GeoDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd207b1d-7b73-4a8f-a1a4-28723f94023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = geojson_to_gdf(out_path, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acb0ec6-eec9-4a33-96fb-a783f2d89cc8",
   "metadata": {},
   "source": [
    "## 4.5. Exporting the GeoDataFrame as Vector Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c716b16a-3392-4fac-b178-0efa66ca9d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as csv\n",
    "clusters.to_csv(os.path.join(out_path,\"{c}.csv\".format(c=result_nm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a247214d-80ec-4a8b-9bba-b2de4b1c9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export as GeoPackage \n",
    "clusters.to_file(os.path.join(out_path,\"{c}.gpkg\".format(c=result_nm)),driver=\"GPKG\")\n",
    "print (\"Part 3 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc706fd-0f12-48f9-be4c-c27446a56748",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 5: Statistics Calculation\n",
    "\n",
    "This part calculates summary statistics for the generated clusters. There outputs include:\n",
    "\n",
    "* Tabular summaries (.csv format) at national level\n",
    "* Tabular summaries (.csv format) grouped by cluster\n",
    "* Interactive graphs (.html) for key attributes per cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0db693-70e7-4c95-8ea8-7cbe0e29cd0b",
   "metadata": {},
   "source": [
    "## 5.1. National Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac175c7-dc10-463d-a867-a486fd120b16",
   "metadata": {},
   "source": [
    "### 5.1.1 Collect Names of Attributes Assigned to the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40422e7-21bb-40f6-901c-81329a495b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_list_of_cols = list(final_clustered_GAEZ_gdf.columns)\n",
    "final_list_of_cols = list(clusters.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55e568-619a-4995-928c-f8f5b79ba0b2",
   "metadata": {},
   "source": [
    "### 5.1.2. Land Cover and Area Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac6b775-e61b-419d-9746-2f76a26b6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Land cover area estimator\n",
    "def calc_LC_sqkm(df, col_list):\n",
    "    \"\"\" \n",
    "    This function takes the df where the LC type for different classes is provided per location (row).\n",
    "    It adds all pixels per location; then is calculates the ratio of LC class in each location (% of total).\n",
    "    Finally is estimates the area per LC type in each location by multiplying with the total area each row represents.\n",
    "    \n",
    "    INPUT: \n",
    "    df -> Pandas dataframe with LC type classification \n",
    "    col_list -> list of columns to include in the summary (e.g. LC1-LC11)\n",
    "    \n",
    "    OUTPUT: Updated dataframe with estimated area (sqkm) of LC types per row\n",
    "    \"\"\"\n",
    "    df[\"LC_sum\"] = df[col_list].sum(axis=1)\n",
    "    for col in col_list:\n",
    "        df[col] = df[col]/df[\"LC_sum\"]*df[\"sqkm\"]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a786784-471f-4742-810d-a08d69d67a05",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Identify land cover related columns\n",
    "landCover_cols = []\n",
    "for col in final_list_of_cols:\n",
    "    if \"LCType\" in col:\n",
    "        landCover_cols.append(col)\n",
    "if not landCover_cols:\n",
    "    print (\"There is not any Land Cover associated column in the dataframe; please revise\")\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae02f59-7b53-4548-9999-8b36a92339bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gdf_LCsqkm = calc_LC_sqkm(clusters, landCover_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37593c7c-ae14-4a82-9637-c21bfe467087",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# List of stast to be calculated\n",
    "lc_sum_rows = ['sum', 'min', 'max']\n",
    "\n",
    "# Initiate the summary table \n",
    "LC_summary_table = pd.DataFrame(index=lc_sum_rows, columns=landCover_cols)\n",
    "\n",
    "# Filling in the table\n",
    "for col in landCover_cols:\n",
    "    LC_summary_table[col][0] = round(data_gdf_LCsqkm[col].sum(),2)\n",
    "    LC_summary_table[col][1] = round(data_gdf_LCsqkm[col].min(),2)\n",
    "    LC_summary_table[col][2] = round(data_gdf_LCsqkm[col].max(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6fa6b3-c874-4177-94d6-ca4e13b849ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('###  These are the summarized results for land cover (sq.km) in **{}**'.format(Full_name)))\n",
    "display(Markdown(' **Total area:** {:0.1f} sq.km'.format(data_gdf_LCsqkm.sqkm.sum())))\n",
    "display(LC_summary_table)\n",
    "display(Markdown('#### Class Description \\n\\n LCType1 : >75% Cropland \\n\\n LCType2 : >75% Tree covered land \\n\\n  LCType3 : >75% Grassland shrub or herbaceous cover \\n\\n LCType4 : >75% Sparsely vegetated or bare \\n\\n LCType5 : 50-75% Cropland \\n\\n LCType6 : 50-75% Tree covered land \\n\\n LCType7 : 50-75% Grassland shrub or herbaceous cover \\n\\n LCType8 : 50-75% Sparsely vegetated or bare \\n\\n LCType9 : >50% Artificial surface \\n\\n LCType10 : Other land cover associations \\n\\n LCType11 : Water permanent snow glacier'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b27d9-a8b2-4031-a073-7190955bb472",
   "metadata": {},
   "source": [
    "### 5.1.3. Other Variables Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6d5db-5b44-4f7e-ac9c-3b1b91984204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for other than land cover attribute columns\n",
    "data_gdf_stat = data_gdf_LCsqkm\n",
    "\n",
    "# Define the conversion factor for CLEWs modelling\n",
    "#Potential yield unit conversion from kg DW/ha to million tonnes per 1000 sqkm\n",
    "factor1 = 0.0001 \n",
    "\n",
    "#Other parameter unit conversion from millimeter to BCM per 1000 sqkm\n",
    "factor2 = 0.001\n",
    "\n",
    "# Multiply each value in the table by the appropriate conversion factor\n",
    "for col in data_gdf_stat.columns:\n",
    "    \n",
    "    if \"yld\" in col:\n",
    "        data_gdf_stat.loc[:, col] *= factor1\n",
    "    elif \"evt\" in col: \n",
    "        data_gdf_stat.loc[:, col] *= factor2\n",
    "    elif \"prc\" in col: \n",
    "        data_gdf_stat.loc[:, col] *= factor2\n",
    "    elif \"cwd\" in col: \n",
    "        data_gdf_stat.loc[:, col] *= factor2     \n",
    "\n",
    "final_list_of_cols = list(data_gdf_stat.columns)\n",
    "\n",
    "sum_cols = [x for x in final_list_of_cols if x not in origin_list_of_cols]\n",
    "sum_cols = [x for x in sum_cols if x not in landCover_cols]\n",
    "sum_cols.remove(\"id\")\n",
    "sum_cols.remove(\"LC_sum\")\n",
    "sum_rows = ['mean', 'min', 'max']\n",
    "\n",
    "other_summary_table = pd.DataFrame(index=sum_rows, columns=sum_cols)\n",
    "\n",
    "for col in sum_cols:\n",
    "    other_summary_table[col][0] = round(data_gdf_stat[col].mean(),4)\n",
    "    other_summary_table[col][1] = round(data_gdf_stat[col].min(),4)\n",
    "    other_summary_table[col][2] = round(data_gdf_stat[col].max(),4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ddfab-0c0a-4f89-9ca4-469195733836",
   "metadata": {},
   "source": [
    "### 5.1.4. Additional crop calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7d633-4def-4c3f-b8a2-e1b57b49460e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional crop calculations is the the average of crops ranked from six to ten in the top 10 FAO dataset\n",
    "additional_crop_stat = [col for col in other_summary_table.columns if any(a in col for a in other_crop_name)]\n",
    "\n",
    "additional_stat_table = other_summary_table.loc[:, additional_crop_stat].copy()\n",
    "\n",
    "other_summary_table = other_summary_table.drop(additional_stat_table, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ef58c-a949-413a-a28d-7a697b21c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new column contain average value of five to ten in the top 10 crops\n",
    "def additional_stat(parameter):\n",
    "    selected = [col for col in additional_stat_table.columns if parameter in col]\n",
    "\n",
    "    selected = additional_stat_table.loc[:, selected]\n",
    "    \n",
    "    Irrigation_Low = selected.loc[:, [a for a in selected.columns if 'Irrigation Low' in a]]\n",
    "    First_Low = round(Irrigation_Low.iloc[0].mean(),4)\n",
    "    Second_Low = round(Irrigation_Low.iloc[1].min(),4)\n",
    "    Third_Low= round(Irrigation_Low.iloc[2].max(),4)\n",
    "    \n",
    "    other_summary_table['OTH'+' '+ parameter +' '+'Irrigation Low_mean'] = 0\n",
    "    other_summary_table['OTH'+' '+ parameter +' '+'Irrigation Low_mean'].iloc[0] = First_Low\n",
    "    other_summary_table['OTH'+' '+ parameter +' '+'Irrigation Low_mean'].iloc[1] =Second_Low\n",
    "    other_summary_table['OTH'+' '+ parameter +' '+'Irrigation Low_mean'].iloc[2] = Third_Low\n",
    "    \n",
    "    Irrigation_High = selected.loc[:, [a for a in selected.columns if 'Irrigation High' in a]]\n",
    "    First_High = round(Irrigation_High.iloc[0].mean(),4)\n",
    "    Second_High = round(Irrigation_High.iloc[1].min(),4)\n",
    "    Third_High= round(Irrigation_High.iloc[2].max(),4)\n",
    "                \n",
    "    other_summary_table['OTH'+' '+ parameter +' '+ 'Irrigation High_mean'] = 0\n",
    "    other_summary_table['OTH'+' '+ parameter +' '+ 'Irrigation High_mean'].iloc[0] = First_High\n",
    "    other_summary_table['OTH'+' '+ parameter +' '+ 'Irrigation High_mean'].iloc[1] = Second_High \n",
    "    other_summary_table['OTH'+' '+ parameter +' '+ 'Irrigation High_mean'].iloc[2] = Third_High    \n",
    "\n",
    "\n",
    "    Rain_fed_Low = selected.loc[:, [a for a in selected.columns if 'Rain-fed Low' in a]]\n",
    "    First_Rain_Low = round(Rain_fed_Low.iloc[0].mean(),4)\n",
    "    Second_Rain_Low = round(Rain_fed_Low.iloc[1].min(),4)\n",
    "    Third_Rain_Low= round(Rain_fed_Low.iloc[2].max(),4)\n",
    "                \n",
    "    other_summary_table['OTH'+' '+ parameter +' '+ 'Rain-fed Low_mean'] = 0 \n",
    "    other_summary_table['OTH'+' '+ parameter +' '+ 'Rain-fed Low_mean'].iloc[0] = First_Rain_Low\n",
    "    other_summary_table['OTH'+' '+ parameter +' '+ 'Rain-fed Low_mean'].iloc[1] = Second_Rain_Low\n",
    "    other_summary_table['OTH'+' '+ parameter +' '+ 'Rain-fed Low_mean'].iloc[2] = Third_Rain_Low\n",
    "                \n",
    "    Rain_fed_High = selected.loc[:, [a for a in selected.columns if 'Rain-fed High' in a]]    \n",
    "    First_Rain_High = round(Rain_fed_High.iloc[0].mean(),4)\n",
    "    Second_Rain_High = round(Rain_fed_High.iloc[1].min(),4)\n",
    "    Third_Rain_High= round(Rain_fed_High.iloc[2].max(),4)\n",
    "                \n",
    "    other_summary_table['OTH'+' '+ parameter +' '+ 'Rain-fed High_mean'] = 0\n",
    "    other_summary_table['OTH'+' '+ parameter +' '+ 'Rain-fed High_mean'].iloc[0] = First_Rain_High\n",
    "    other_summary_table['OTH'+' '+ parameter +' '+ 'Rain-fed High_mean'].iloc[1] = Second_Rain_High\n",
    "    other_summary_table['OTH'+' '+ parameter +' '+ 'Rain-fed High_mean'].iloc[2] = Third_Rain_High   \n",
    "\n",
    "additional_stat('yld')\n",
    "additional_stat('cwd')\n",
    "additional_stat('evt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a7b9d-4b38-4e7a-8dc8-5562f399ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('###  \\n These are the summarized results for the other variables variables collected for **{}**'.format(Full_name)))\n",
    "display(other_summary_table)\n",
    "display(Markdown('### Note! \\n Units presented in this analysis are based on the CLEWs modelling framework.  The million tonnes per 1000 km² unit of measurement for agro-climatic potential yield. BCM (billion cubic meters) per 1000 km² is used to measure crop water deficit, crop evapotranspiration, and precipitation. These units have been chosen to ensure consistency with the CLEWs modelling methodology and facilitate comparability with other studies .'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e092605-349a-4166-896f-986c1e9de299",
   "metadata": {},
   "source": [
    "### 5.1.5. Exporting National Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db167672-4264-4aea-ab02-f7d893909c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Export national stats to csv\n",
    "LC_summary_table.to_csv(os.path.join(summary_stats_path,\"{}_LandCover_National_summary.csv\".format(country_name)))\n",
    "other_summary_table.to_csv(os.path.join(summary_stats_path,\"{}_Parameter_National_summary.csv\".format(country_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff48ab2-b02d-4943-b737-0c3aa6106e58",
   "metadata": {},
   "source": [
    "## 5.2. Calculating Cluster Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b209ef9-a4d5-42d6-88e5-2ec047ce4f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gdf_stat[\"cluster\"] = data_gdf_stat[\"cluster\"].astype(str)\n",
    "non_clustered_data = data_gdf_stat[data_gdf_stat[\"cluster\"] == \"None\"]\n",
    "\n",
    "display(Markdown('**Note** that there are {} polygons that are not assigned to a cluster  -- classified as \"None\"'\n",
    "                 .format(len(non_clustered_data))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0345864a-ab78-441f-9de4-e9fd3717cb7d",
   "metadata": {},
   "source": [
    "### 5.2.1. Grouping Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93224311-79f2-4a36-a3d6-2094350014bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = data_gdf_stat.groupby(['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f6297-7217-42e8-9733-b5d259fde3fa",
   "metadata": {},
   "source": [
    "### 5.2.2. Land Cover and Area Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b26fa-a51d-4918-8322-80c540535ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_lc = clusters[landCover_cols].sum().merge(clusters[\"sqkm\"].sum(), on=\"cluster\").round(decimals = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd5d003-4b16-420c-a3b6-5925d4b9214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_lc.sort_values(ascending=False, by='sqkm').reset_index()\n",
    "display(Markdown('#### Cluster summary statistics for area and land cover in {}'.format(Full_name)))\n",
    "display(Markdown(' **Total area:** {:0.1f} sq.km'.format(clusters_lc.sqkm.sum())))\n",
    "clusters_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b67eb0-3272-41d5-9bcd-b732ac44614d",
   "metadata": {},
   "source": [
    "### 5.2.3. Aggregating Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a51f9f1-d9d8-4f7c-9869-ae3d2cdcad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if aggregate: \n",
    "    if Extract:\n",
    "        # Extract the row with index \"TAI\" from clusters_lc and store it in a new dataframe\n",
    "        Ext_cluster = clusters_lc.loc[clusters_lc.index == Ext_region]\n",
    "        # Exclude the row with index \"TAI\" from clusters_lc\n",
    "        clusters_lc = clusters_lc.loc[clusters_lc.index != Ext_region]\n",
    "    \n",
    "    # Split the index of the clusters into groups of 10 rows\n",
    "    cluster_groups = [clusters_lc.index[i:i+region_per_group] for i in range(0, len(clusters_lc), region_per_group)]\n",
    "    \n",
    "    # Create a new DataFrame to store the aggregated values for each new cluster\n",
    "    new_clusters = pd.DataFrame(columns=landCover_cols+[\"sqkm\"])\n",
    "    \n",
    "    # Iterate over each cluster group, calculate the sum of values and add it to the new DataFrame with the new cluster name\n",
    "    for i, group in enumerate(cluster_groups):\n",
    "        new_cluster_name = \"NC\" + chr(ord('A') + i)\n",
    "        new_cluster_values = clusters_lc.loc[group].sum()\n",
    "        new_clusters.loc[new_cluster_name] = new_cluster_values\n",
    "        \n",
    "        # Print the old cluster names and their allocation to the new clusters\n",
    "    for i, group in enumerate(cluster_groups):\n",
    "        old_cluster_names = ', '.join([str(name) for name in group])\n",
    "        new_cluster_name = \"NC\" + chr(ord('A') + i)\n",
    "        print(f\"Old clusters {old_cluster_names} are allocated to new cluster {new_cluster_name}\")\n",
    "            \n",
    "    if Extract:\n",
    "        # Adding the excluded region \n",
    "        merged_df = pd.concat([new_clusters, Ext_cluster], axis=0)\n",
    "        clusters_lc=merged_df\n",
    "    else:\n",
    "        clusters_lc=new_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7d0eb-84d7-48ab-956f-a5fe34e0c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_lc.sort_values(ascending=False, by='sqkm').reset_index()\n",
    "display(Markdown('#### Aggregated cluster summary statistics for area and land cover in {}'.format(Full_name)))\n",
    "display(Markdown(' **Total area:** {:0.1f} sq.km'.format(clusters_lc.sqkm.sum())))\n",
    "clusters_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25daac0a-ea80-4320-a2c9-9b58946d90a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export cluster stats to csv\n",
    "clusters_lc.to_csv(os.path.join(summary_stats_path,\"{}_LandCover_byCluster_summary.csv\".format(country_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4fafaa-4d3d-4ccf-85bc-13305116e6d4",
   "metadata": {},
   "source": [
    "### 5.2.4. Other Variable Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee98e82a-93d6-4d6a-9ea6-bf6067614d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_stat = clusters[sum_cols].mean().round(decimals = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129b960-e543-409f-8736-be3cb9108db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional crop calculation is the average of crops ranked from five to ten in the top 10 FAO dataset\n",
    "additional_crop_stat_group = [col for col in clusters_stat.columns if any(a in col for a in other_crop_name)]\n",
    "additional_stat_table_group = clusters_stat.loc[:, additional_crop_stat_group].copy()\n",
    "clusters_stat = clusters_stat.drop(additional_stat_table_group, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dcf92d-a50e-405c-96f1-c565a3340292",
   "metadata": {},
   "source": [
    "### 5.2.5. Calculating the Average of Additional Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbf90f-af5f-413b-a595-dd420ecd364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add new column contain average value of five to ten in the top 10 crops\n",
    "def additional_stat_group(parameter):\n",
    "    selected_group = [col for col in additional_stat_table_group.columns if parameter in col]\n",
    "\n",
    "    selected_group = additional_stat_table_group.loc[:, selected_group]\n",
    "    \n",
    "    Irrigation_Low_group = selected_group.loc[:, [a for a in selected_group.columns if 'Irrigation Low' in a]]\n",
    "    Low_group = round(Irrigation_Low_group.mean(axis=1),4)\n",
    "    clusters_stat['OTH'+' '+ parameter +' '+'Irrigation Low_mean'] = Low_group\n",
    "    \n",
    "    Irrigation_High_group = selected_group.loc[:, [a for a in selected_group.columns if 'Irrigation High' in a]]\n",
    "    High_group = round(Irrigation_High_group.mean(axis=1),4)            \n",
    "    clusters_stat['OTH'+' '+ parameter +' '+ 'Irrigation High_mean'] = High_group    \n",
    "\n",
    "\n",
    "    Rain_fed_Low_group = selected_group.loc[:, [a for a in selected_group.columns if 'Rain-fed Low' in a]]   \n",
    "    Rain_Low = round(Rain_fed_Low_group.mean(axis=1),4)            \n",
    "    clusters_stat['OTH'+' '+ parameter +' '+ 'Rain-fed Low_mean'] = Rain_Low \n",
    "                \n",
    "    Rain_fed_High_group = selected_group.loc[:, [a for a in selected_group.columns if 'Rain-fed High' in a]]         \n",
    "    Rain_High = round(Rain_fed_High_group.mean(axis=1),4)               \n",
    "    clusters_stat['OTH'+' '+ parameter +' '+ 'Rain-fed High_mean'] =Rain_High  \n",
    "\n",
    "additional_stat_group('yld')\n",
    "additional_stat_group('cwd')\n",
    "additional_stat_group('evt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9867423-547c-4f2f-ace5-3c08795f6e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('#### Cluster summary statistics for other variables in {}'.format(Full_name)))\n",
    "clusters_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d885a-4188-45e7-9fc0-dfaf87206267",
   "metadata": {},
   "source": [
    "### 5.2.6. Aggregating Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a07305-9d5f-4b1e-bb38-ad7e56e89702",
   "metadata": {},
   "outputs": [],
   "source": [
    "if aggregate: \n",
    "    if Extract:\n",
    "        # Extract the row with index \"TAI\" from clusters_stat and store it in a new dataframe\n",
    "        Ext_cluster_stat = clusters_stat.loc[clusters_stat.index == Ext_region]\n",
    "        # Exclude the row with index \"TAI\" from clusters_stat\n",
    "        clusters_stat = clusters_stat.loc[clusters_stat.index != Ext_region]\n",
    "    \n",
    "    # Split the index of the clusters into groups of 10 rows\n",
    "    clusters_stat_groups = [clusters_stat.index[i:i+region_per_group] for i in range(0, len(clusters_stat), region_per_group)]\n",
    "    \n",
    "    # Create a new DataFrame to store the aggregated values for each new cluster\n",
    "    new_clusters_stat = pd.DataFrame(columns=clusters_stat.columns)\n",
    "    \n",
    "    display(Markdown('#### Cluster summary statistics for other variables in {}'.format(country_name)))\n",
    "    new_clusters_stat\n",
    "    \n",
    "    new_list = list(other_summary_table.columns)\n",
    "    \n",
    "    # Iterate over each cluster group, calculate the sum of values and add it to the new DataFrame with the new cluster name\n",
    "    for i, group in enumerate(clusters_stat_groups):\n",
    "        new_clusters_stat_name = \"NC\" + chr(ord('A') + i)\n",
    "        group_data = clusters_stat.loc[group, new_list]\n",
    "        group_mean = group_data.mean().round(decimals=4)\n",
    "        new_clusters_stat.loc[new_clusters_stat_name] = group_mean\n",
    "        \n",
    "    for i, group in enumerate(clusters_stat_groups):\n",
    "        old_clusters_stat_names = ', '.join([str(name) for name in group])\n",
    "        new_clusters_stat_name = \"NC\" + chr(ord('A') + i)\n",
    "        print(f\"Old clusters {old_clusters_stat_names} are allocated to new cluster {new_clusters_stat_name}\")\n",
    "            \n",
    "    if Extract:\n",
    "        # Adding the excluded region \n",
    "        merged_df = pd.concat([new_clusters_stat, Ext_cluster_stat], axis=0)\n",
    "        clusters_stat=merged_df\n",
    "    else:\n",
    "        clusters_stat=new_clusters_stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ef978-4a7b-49c4-9345-08f6d9fabd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown('#### Aggregated cluster summary statistics for other variables in {}'.format(Full_name)))\n",
    "clusters_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8501b5d-36bb-4ac2-b15c-39bbe6aedc7b",
   "metadata": {},
   "source": [
    "## 5.3. Generate clewsy-compatible Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147d8c0-b4cc-4590-af15-444d0d34eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_other = clusters_stat\n",
    "for index, row in clusters_other.iterrows():\n",
    "    row_h = row.to_frame().T\n",
    "    \n",
    "    # generating the crop potential yeild csv files\n",
    "    yld_columns = [col for col in row_h.columns if 'yld' in col]\n",
    "    yld_df = row_h[yld_columns]\n",
    "\n",
    "    # Name correction\n",
    "    yld_rename = {col: col.replace(' yld', '').replace('_mean', '') for col in yld_df.columns}\n",
    "    yld_df = yld_df.rename(columns=yld_rename)\n",
    "\n",
    "    empty_yld = pd.DataFrame(columns=['cluster', '', '', '', '','', '', '', '', ''])\n",
    "    combined_yld = pd.concat([empty_yld, yld_df], axis=1).reset_index(drop=True)\n",
    "    combined_yld.loc[0, 'cluster'] = 1\n",
    "    combined_yld.to_csv(os.path.join(summary_stats_path,\"clustering_results_{}.csv\".format(index)), index=False)\n",
    "    \n",
    "    # generating crop evapotranspiration csv files\n",
    "    evt_columns = [col for col in row_h.columns if 'evt' in col]\n",
    "    evt_df = row_h[evt_columns]\n",
    "    \n",
    "    # Name correction\n",
    "    evt_rename = {col: col.replace(' evt', '').replace('_mean', '') for col in evt_df.columns}\n",
    "    evt_df = evt_df.rename(columns=evt_rename)\n",
    "    \n",
    "    empty_evt = pd.DataFrame(columns=['cluster', '', '', '', '','', '', '', '', ''])\n",
    "    combined_evt = pd.concat([empty_evt, evt_df], axis=1).reset_index(drop=True)\n",
    "    combined_evt.loc[0, 'cluster'] = 1\n",
    "    combined_evt.to_csv(os.path.join(summary_stats_path,\"clustering_results_evt_{}.csv\".format(index)), index=False)\n",
    "    \n",
    "    #generating crop water deficit csv files \n",
    "    cwd_columns = [col for col in row_h.columns if 'cwd' in col]\n",
    "    cwd_df = row_h[cwd_columns]\n",
    "    \n",
    "    # Name correction\n",
    "    cwd_rename = {col: col.replace(' cwd', '').replace('_mean', '') for col in cwd_df.columns}\n",
    "    cwd_df = cwd_df.rename(columns=cwd_rename)\n",
    "    \n",
    "    empty_cwd = pd.DataFrame(columns=['cluster', '', '', '', '','', '', '', '', ''])\n",
    "    combined_cwd = pd.concat([empty_cwd, cwd_df], axis=1).reset_index(drop=True)\n",
    "    combined_cwd.loc[0, 'cluster'] = 1\n",
    "    combined_cwd.to_csv(os.path.join(summary_stats_path,\"clustering_results_cwd_{}.csv\".format(index)), index=False)\n",
    "    \n",
    "    #  generating precipitation csv files\n",
    "    prc_columns = [col for col in row_h.columns if 'prc' in col]\n",
    "    prc_df = row_h[prc_columns]\n",
    "    \n",
    "    prc_rename = {col: col.replace(' prc', '').replace('_mean', '') for col in prc_df.columns}\n",
    "    prc_df = prc_df.rename(columns=prc_rename)\n",
    "    \n",
    "    empty_prc = pd.DataFrame(columns=['cluster'])\n",
    "    combined_prc = pd.concat([empty_prc, prc_df], axis=1).reset_index(drop=True)\n",
    "    combined_prc.loc[0, 'cluster'] = 1\n",
    "    combined_prc.to_csv(os.path.join(summary_stats_path,\"clustering_results_prc_{}.csv\".format(index)), index=False)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513c94ac-760c-492f-a46f-23efcaa94fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export national stats to csv\n",
    "clusters_other.to_csv(os.path.join(summary_stats_path,\"{}_Parameter_byCluster_summary.csv\".format(country_name)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfbdf11-8ca8-4570-8128-288ff3f7dd3b",
   "metadata": {},
   "source": [
    "### 5.4. Generate Interactive Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd441fac-9a1a-4b17-b53b-8d53396cfc4f",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_interactive_graph_sum(clust_dict, parameter, name):\n",
    "    for key, value in clust_dict.items():\n",
    "        clust_dict[key] = round(clusters.get_group(key)[parameter].sum(),2)\n",
    "    fig_Cluster = px.bar(pd.DataFrame.from_dict(clust_dict, orient='index', columns=[\"sum\"]), title=\"Distribution of {} over clusters in {}\".format(parameter, Full_name))\n",
    "    #fig_Cluster.show()\n",
    "    # Export figure as html\n",
    "    fig_Cluster.write_html((os.path.join(summary_stats_path,\"{}_{}_{}_perCluster.html\".format(name, parameter, \"sum\"))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d9298-622c-41c1-89c4-ad786599f9da",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_interactive_graph_mean(clust_dict, parameter, name):\n",
    "    for key, value in clust_dict.items():\n",
    "        clust_dict[key] = round(clusters.get_group(key)[parameter].mean(),2)\n",
    "    fig_Cluster = px.bar(pd.DataFrame.from_dict(clust_dict, orient='index', columns=[\"mean\"]), title=\"Distribution of {} over clusters in {}\".format(parameter, Full_name))\n",
    "    #fig_Cluster.show()\n",
    "    # Export figure as html\n",
    "    fig_Cluster.write_html((os.path.join(summary_stats_path,\"{}_{}_{}_perCluster.html\".format(name, parameter, \"mean\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060f62f-4328-4faf-9e89-98887414a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster names\n",
    "clust_names = list(data_gdf_stat.cluster.unique())\n",
    "\n",
    "# Create a dictionary that includes the name of the clusters and a selected parameter\n",
    "clust_dict = dict.fromkeys(clust_names, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687d347-5e70-408d-94f8-55040ee291ba",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "landCover_cols.append(\"sqkm\")\n",
    "\n",
    "for item in landCover_cols:\n",
    "    make_interactive_graph_sum(clust_dict, item, country_name) \n",
    "    \n",
    "for col in sum_cols:\n",
    "    make_interactive_graph_mean(clust_dict, col, country_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef67bb-0398-45fa-b7bf-5c36855fba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Part 5 - and with that the analysis - completed!\")\n",
    "print (\"Total elapsed time: {}\".format(time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
